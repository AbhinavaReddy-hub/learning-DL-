{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbhinavaReddy-hub/learning-DL-/blob/main/LSTM_nextword_prediction__exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rJ6xjlOzxCNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "v-xl6ckVVuV_"
      },
      "source": [
        "# Predicting the next word from DataFrame using LSTM in PyTorch\n",
        "In this exercise we will build and train aLSTM Network to predict the next word based on sample data."
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "-L0ZrCkzVuWE"
      },
      "source": [
        "To build an LSTM model for word prediction using PyTorch, follow these steps:\n",
        "\n",
        "1. Data Preparation\n",
        "    1.Import Necessary libraries\n",
        "    2.Create a DataFrame with sample text.\n",
        "    3.Preprocess the data:\n",
        "       1.tokenize the text, removing punctuation and converting all text to lowercase.\n",
        "       2. convert to word embeddings-vector representation of words\n",
        "       3. Prepare the sequences for the LSTM.\n",
        "       4. convert to pytorch tensors\n",
        "2. Define the LSTM Model\n",
        "    1.Use nn.LSTM/nn.GRU from PyTorch.\n",
        "    2.Initialize the model-Include input size, hidden size, vocabulary size.\n",
        "3. Training the Model\n",
        "    1.Define a loss function, e.g., Cross Entropy Loss.\n",
        "    2.Use an optimizer like Adam.\n",
        "    3.Train the model over multiple epochs.\n",
        "4. Evaluate the Model\n",
        "    1.Test the model - interactive testing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sylZ_F05VuWF"
      },
      "source": [
        "# 1.Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LP_23ffJVuWG"
      },
      "source": [
        "## 1.Import necessary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YfyRBvFZGiMw"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from gensim.models import Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0BM8E8eVuWI"
      },
      "source": [
        "# 2.Create a DataFrame with sample text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ItUE6jYVG2Wn"
      },
      "outputs": [],
      "source": [
        "# Step 1: Create an example dataset\n",
        "data = {\n",
        "    \"text\": [\n",
        "        \"The movie was fantastic and very engaging\",\n",
        "        \"I hated the acting and the storyline\",\n",
        "        \"It was boring and lacked depth\",\n",
        "        \"Amazing performance by the actors and great direction\",\n",
        "        \"Not worth watching at all\",\n",
        "        \"One of the best movies I have ever seen\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Create a DataFrame from the dataset\n",
        "df = pd.DataFrame(data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrB7r4T1VuWJ"
      },
      "source": [
        "# 3.Preprocess the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxWrTvEKVuWJ"
      },
      "source": [
        "# Step 1: Tokenize the text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vea28q-EG66e",
        "outputId": "98ac3507-dc6b-40a9-8da2-8a4c9197aaab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text  \\\n",
            "0          The movie was fantastic and very engaging   \n",
            "1               I hated the acting and the storyline   \n",
            "2                     It was boring and lacked depth   \n",
            "3  Amazing performance by the actors and great di...   \n",
            "4                          Not worth watching at all   \n",
            "5            One of the best movies I have ever seen   \n",
            "\n",
            "                                              tokens  \n",
            "0  [the, movie, was, fantastic, and, very, engaging]  \n",
            "1       [i, hated, the, acting, and, the, storyline]  \n",
            "2              [it, was, boring, and, lacked, depth]  \n",
            "3  [amazing, performance, by, the, actors, and, g...  \n",
            "4                    [not, worth, watching, at, all]  \n",
            "5  [one, of, the, best, movies, i, have, ever, seen]  \n"
          ]
        }
      ],
      "source": [
        "# Step 1: Define a function to tokenize the text\n",
        "def tokenize(text):\n",
        "    return re.findall(r'\\b\\w+\\b', text.lower())\n",
        "# Apply the tokenization function to the text column\n",
        "df['tokens'] = df['text'].apply(tokenize)\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKAtRCQ5VuWK"
      },
      "source": [
        "# step-2: word embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdJ7aaI8G_H2",
        "outputId": "a724dd72-c843-4cf3-a94e-7c9c2cbe0eb6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word2Vec<vocab=33, vector_size=50, alpha=0.025> Word2Vec<vocab=33, vector_size=50, alpha=0.025>\n"
          ]
        }
      ],
      "source": [
        "# Step 2: Train a Word2Vec model on the tokenized sentences\n",
        "word2vec_model = Word2Vec(sentences=df['tokens'], vector_size=50, window=3, min_count=1)\n",
        "print(word2vec_model,word2vec_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvtefA8YVuWK"
      },
      "source": [
        "## Step 3: Prepare the sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pum4r6ZXHIjv"
      },
      "outputs": [],
      "source": [
        "# Step 3: Prepare dataset for word prediction\n",
        "def prepare_sequences(tokens, model, context_size=3):\n",
        "    X, y = [], []\n",
        "    for i in range(len(tokens) - context_size):\n",
        "        # Context words\n",
        "        context = tokens[i:i + context_size]\n",
        "        # Target word\n",
        "        target = tokens[i + context_size]\n",
        "        X.append([model.wv[word] for word in context])\n",
        "        y.append(model.wv.key_to_index[target])  # Index of the target word in vocabulary\n",
        "    return np.array(X), np.array(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hyamphaqHOZY",
        "outputId": "31f4c37c-7d4f-450b-d0e9-ade125a43c9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(24, 3, 50) (24,)\n",
            "[[[-0.00108157  0.00046619  0.01021299 ...  0.01921763  0.00997272\n",
            "    0.01847972]\n",
            "  [ 0.00287149 -0.00529305 -0.01414844 ...  0.0010219   0.01642843\n",
            "   -0.01403542]\n",
            "  [-0.01724348  0.00732895  0.01037926 ... -0.00309765  0.00302098\n",
            "    0.00358537]]\n",
            "\n",
            " [[ 0.00287149 -0.00529305 -0.01414844 ...  0.0010219   0.01642843\n",
            "   -0.01403542]\n",
            "  [-0.01724348  0.00732895  0.01037926 ... -0.00309765  0.00302098\n",
            "    0.00358537]\n",
            "  [-0.01648536  0.01859871 -0.00039532 ... -0.00476074 -0.0062565\n",
            "   -0.00474028]]\n",
            "\n",
            " [[-0.01724348  0.00732895  0.01037926 ... -0.00309765  0.00302098\n",
            "    0.00358537]\n",
            "  [-0.01648536  0.01859871 -0.00039532 ... -0.00476074 -0.0062565\n",
            "   -0.00474028]\n",
            "  [-0.01629744  0.00898925 -0.00828098 ... -0.01408151  0.00179921\n",
            "    0.01281321]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 0.00660615  0.01019108  0.00917083 ...  0.0165313  -0.01220133\n",
            "    0.0189127 ]\n",
            "  [-0.0144051   0.00845618  0.00434286 ... -0.0190672   0.00316558\n",
            "   -0.01960658]\n",
            "  [ 0.01562445 -0.01901143 -0.00039841 ... -0.00477906 -0.01902632\n",
            "    0.0090201 ]]\n",
            "\n",
            " [[-0.0144051   0.00845618  0.00434286 ... -0.0190672   0.00316558\n",
            "   -0.01960658]\n",
            "  [ 0.01562445 -0.01901143 -0.00039841 ... -0.00477906 -0.01902632\n",
            "    0.0090201 ]\n",
            "  [-0.00977624 -0.00691614  0.01923026 ...  0.01887651  0.01411623\n",
            "    0.01354753]]\n",
            "\n",
            " [[ 0.01562445 -0.01901143 -0.00039841 ... -0.00477906 -0.01902632\n",
            "    0.0090201 ]\n",
            "  [-0.00977624 -0.00691614  0.01923026 ...  0.01887651  0.01411623\n",
            "    0.01354753]\n",
            "  [-0.00010563  0.00053871 -0.00033761 ...  0.01423649  0.01178288\n",
            "   -0.01116123]]] [ 6  1  7  8 10  1  0 11  1 14 15  0 19  1 20 21 25 26 29 30  3 31 17 16]\n"
          ]
        }
      ],
      "source": [
        "# Create sequences for all rows in the dataset\n",
        "context_size = 3\n",
        "X, y = [], []\n",
        "for tokens in df['tokens']:\n",
        "    X_seq, y_seq = prepare_sequences(tokens, word2vec_model, context_size)\n",
        "    X.extend(X_seq)\n",
        "    y.extend(y_seq)\n",
        "\n",
        "X, y = np.array(X), np.array(y)\n",
        "print(X.shape,y.shape)\n",
        "print(X,y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFOluKfpVuWL"
      },
      "source": [
        "# Step 4: Convert to pytorch tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBOFLYKvVuWL"
      },
      "outputs": [],
      "source": [
        "# Step 4: Convert data to PyTorch tensors\n",
        "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "y_tensor = torch.tensor(y, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7mMGgJyVuWL"
      },
      "source": [
        "# 2.LSTM Model Building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mv3eBMiTVuWL"
      },
      "source": [
        "# Step 1 Define the LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XnH_jSnpVuWM"
      },
      "outputs": [],
      "source": [
        "# Step 1: Define the LSTM model for word prediction\n",
        "class WordPredictionLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, vocab_size):\n",
        "        super(WordPredictionLSTM, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, (hidden, _) = self.lstm(x)  # Get the hidden state from LSTM\n",
        "        out = self.fc(hidden[-1])  # Pass hidden state through a fully connected layer\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muaRrMqqVuWM"
      },
      "source": [
        "# step-2:Intialize the model parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5CIqdGdbHSqz"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Step 2: Initialize the model\n",
        "input_size = 50  # Size of the word vector\n",
        "hidden_size = 64  # Number of hidden units in LSTM\n",
        "vocab_size = len(word2vec_model.wv)  # Vocabulary size\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khlIq8xKVuWM"
      },
      "source": [
        "# step-2: Initialize model, loss function and optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DP9YfUwHa4s"
      },
      "outputs": [],
      "source": [
        "# Step 1: Define loss function and optimizer\n",
        "model = WordPredictionLSTM(input_size, hidden_size, vocab_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYL-PqqRVuWM"
      },
      "source": [
        "# 3. Training the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVRb--9jVuWM"
      },
      "source": [
        "# step-1: Train the model over multiple epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLKtF1rsHeWk",
        "outputId": "e76b0d61-8f3f-4865-844f-47e1f8251589",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 33])\n",
            "torch.Size([24])\n"
          ]
        }
      ],
      "source": [
        "# Step 2: Training loop\n",
        "num_epochs = 200  # Number of epochs for training\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    optimizer.zero_grad()  # Reset gradients\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(X_tensor)  # Get model predictions\n",
        "    print(outputs.shape)\n",
        "    print(y_tensor.shape)\n",
        "    loss = criterion(outputs, y_tensor)  # Calculate loss\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    loss.backward()  # Backpropagation\n",
        "    optimizer.step()  # Update model parameters\n",
        "\n",
        "    # print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCGoWBYpVuWN"
      },
      "source": [
        "# 4.Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KnzLiv6cHfJM"
      },
      "outputs": [],
      "source": [
        "# Step 1: Prediction function for next word\n",
        "def predict_next_word(context, model, word2vec_model, context_size=3):\n",
        "    # Tokenize the input context\n",
        "    tokens = tokenize(context)\n",
        "    if len(tokens) < context_size:\n",
        "        raise ValueError(f\"Context must have at least {context_size} words\")\n",
        "    tokens = tokens[-context_size:]  # Use only the last `context_size` words\n",
        "\n",
        "    # Convert tokens to vectors\n",
        "    vectors = [word2vec_model.wv[word] for word in tokens]\n",
        "\n",
        "    # Convert to PyTorch tensor and add batch dimension\n",
        "    input_tensor = torch.tensor([vectors], dtype=torch.float32)  # Shape: (1, context_size, input_size)\n",
        "\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Make prediction\n",
        "    with torch.no_grad():\n",
        "        output = model(input_tensor)  # Get model predictions\n",
        "        predicted_index = torch.argmax(output, dim=1).item()  # Get the index of the maximum value (class)\n",
        "\n",
        "    # Convert index back to word\n",
        "    predicted_word = word2vec_model.wv.index_to_key[predicted_index]\n",
        "    return predicted_word"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLh_mvLHVuWO"
      },
      "source": [
        "# Interactive testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1iXNNlXHoEf"
      },
      "outputs": [],
      "source": [
        "# Interactive Testing Function\n",
        "def interactive_predict(model, word2vec_model, context_size=3):\n",
        "    print(\"\\nInteractive Word Prediction\")\n",
        "    print(\"Enter a context sentence to predict the next word.\")\n",
        "    print(\"Type 'exit' to quit.\\n\")\n",
        "\n",
        "    while True:\n",
        "        context = input(\"Enter context: \")\n",
        "        if context.lower() == 'exit':\n",
        "            print(\"Exiting interactive testing. Goodbye!\")\n",
        "            break\n",
        "\n",
        "        try:\n",
        "            next_word = predict_next_word(context, model, word2vec_model, context_size)\n",
        "            print(f\"Predicted next word: \\\"{next_word}\\\"\")\n",
        "        except ValueError as e:\n",
        "            print(f\"Error: {e}. Ensure the context has at least {context_size} words.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZWQmtDlVuWO",
        "outputId": "4d55bb78-ebed-4fc9-bda8-802bf552b0cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Interactive Word Prediction\n",
            "Enter a context sentence to predict the next word.\n",
            "Type 'exit' to quit.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Run interactive testing\n",
        "interactive_predict(model, word2vec_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0nmRc3-VuWO"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "toc-autonumbering": true,
    "toc-showcode": false
  },
  "nbformat": 4,
  "nbformat_minor": 0
}